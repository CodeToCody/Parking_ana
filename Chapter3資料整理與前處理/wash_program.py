import os
import pandas as pd
import numpy as np
from datetime import datetime

# è¨­å®šæ ¼å¼
datetime_format = "%Y/%m/%d"

magic_num = 0
# 0 for notebook
# 1 for desktop


directory_path = os.getcwd()

if magic_num == 0:
    input_file_path = r"C:/Cody/Research/source"
    output_file = r"C:/Cody/Research/clean_data/prepare.csv"
elif magic_num == 1:
    input_file_path = r"D:/Research/source"
    output_file = r"D:/Research/clean_data/prepare.csv"
else:
    print("wrong magic num.")
    exit(1)


# è‡ªè¨‚æ¬„ä½é †åºï¼ˆç§»é™¤ã€Œç´¢å¼•ã€æ¬„ä½ï¼‰
column_names = ["è»Šè™Ÿ", "ç¥¨ç¨®", "å­å ´ç«™", "é€²å‡ºåŠä»˜è²»ç‹€æ…‹", "æ ¡æ­£ç‹€æ…‹", 
                "é€²ç«™è¨­å‚™", "å‡ºç«™è¨­å‚™", "é€²å…¥æ—¥", "é€²å…¥æ™‚é–“", "å‡ºå ´æ—¥", 
                "å‡ºå ´æ™‚é–“", "åœç•™æ™‚æ•¸", "è¨ˆåƒ¹ä»£ç¢¼", "ç´€éŒ„æ™‚é–“"]

# ç¥¨ç¨®å°æ‡‰å­—å…¸
ticket_type_mapping = {
    "æ•™è·æ±½è»Šè­‰": "æ•™è·å“¡æ±½è»Š",
    "æ•™è·è¨ˆæ¬¡è­‰": "æ•™è·å“¡è¨ˆæ¬¡",
    "å­¸ç”Ÿé•·æ™‚æ±½è»Šè­‰": "å­¸ç”Ÿé•·æ™‚æ±½è»Š",
    "å­¸ç”Ÿè¨ˆæ¬¡è­‰": "å­¸ç”Ÿè¨ˆæ¬¡æ±½è»Š",
    "å» å•†æ±½è»Š": "é•·æ™‚å» å•†æ±½è»Š",
    "é€€ä¼‘æ ¡å‹è­‰": "é€€ä¼‘åŠæ ¡å‹æ±½è»Šè­˜åˆ¥è­‰",
    "åœ¨è·å°ˆç­è­‰": "åœ¨è·å°ˆç­æ±½è»Š"
}

# **è¼‰å…¥è»Šè™Ÿå°æ‡‰ç¥¨ç¨®**
def load_ticket_mapping(ticket_path):
    """ è¼‰å…¥è»Šè™Ÿ -> ç¥¨ç¨®çš„æ­£ç¢ºå°æ‡‰è¡¨ """
    ticket_mapping = {}
    try:
        excel_data = pd.ExcelFile(ticket_path,engine="xlrd")

        for sheet_name in excel_data.sheet_names:
            df = pd.read_excel(ticket_path, sheet_name=sheet_name, header=2, dtype=str)    # è®€å–ç¬¬3åˆ—å¾Œçš„è³‡æ–™
            if "è»Šè™Ÿ" in df.columns:
                df["è»Šè™Ÿ"] = df["è»Šè™Ÿ"].str.replace("-", "", regex=True).str.strip()         # å»é™¤ `-`
                mapped_ticket_type = ticket_type_mapping.get(sheet_name, sheet_name)        # å¦‚æœæ²’æœ‰å°æ‡‰ï¼Œä¿æŒåŸç¥¨ç¨®å
                df["ç¥¨ç¨®"] = mapped_ticket_type
                ticket_mapping.update(df.set_index("è»Šè™Ÿ")["ç¥¨ç¨®"].to_dict())
        print(f"æˆåŠŸè¼‰å…¥ {len(ticket_mapping)} ç­†è»Šè™Ÿå°æ‡‰ç¥¨ç¨®è³‡æ–™")
    except Exception as e:
        print(f"è®€å–ç¥¨ç¨®è³‡æ–™å¤±æ•—: {e}")

    return ticket_mapping

# **è™•ç†ç¥¨ç¨®æ ¡æ­£**
def correct_ticket_type(row, mapping1,mapping2):
    """ æ ¡æ­£ 8/1 ä¹‹å‰çš„å°æ‡‰ç¥¨ç¨® """
    car_number = row["è»Šè™Ÿ"].strip().upper() # æ¨™æº–åŒ–

    # ç¢ºä¿é€²å…¥æ—¥ç‚º datatime ç‰©ä»¶
    try:
        entry_date = datetime.strptime(row["é€²å…¥æ—¥"],"%Y/%m/%d")
    except ValueError:
        return row # æ—¥æœŸæœ‰èª¤(æ ¼å¼éŒ¯èª¤)ï¼Œè·³éä¿®æ­£
    
    # æ ¡æ­£ 8/1 å‰çš„ç´€éŒ„
    cutoff_date = datetime(entry_date.year, 8, 1)  # è¨­å®š 8/1 ä½œç‚ºåˆ†ç•Œé»

    # **8/1 ä¹‹å‰ â†’ ç”¨ `mapping1` æ ¡æ­£**
    if entry_date < cutoff_date and car_number in mapping1:
        expected_ticket = mapping1[car_number]
        if row["ç¥¨ç¨®"] != expected_ticket:
            # print(f"ğŸš— ä¿®æ­£è»Šè™Ÿ {car_number} (8/1 å‰): ç¥¨ç¨®å¾ {row['ç¥¨ç¨®']} â†’ {expected_ticket}")
            row["ç¥¨ç¨®"] = expected_ticket

    # **8/1 ä¹‹å¾Œ â†’ ç”¨ `mapping2` æ ¡æ­£**
    elif entry_date >= cutoff_date and car_number in mapping2:
        expected_ticket = mapping2[car_number]
        if row["ç¥¨ç¨®"] != expected_ticket:
            # print(f"ğŸš— ä¿®æ­£è»Šè™Ÿ {car_number} (8/1 å¾Œ): ç¥¨ç¨®å¾ {row['ç¥¨ç¨®']} â†’ {expected_ticket}")
            row["ç¥¨ç¨®"] = expected_ticket

    return row

# **è¼¸å…¥æµå‡½å¼**ï¼ˆè®€å–å¤šå€‹ Excel æª”æ¡ˆï¼‰
'''

    *** ç”¢å‡ºæª”æ¡ˆ: all_data  ***

    åŒ…å«å…¨å¹´æˆ–æ˜¯åŒä¸€å€‹è³‡æ–™å¤¾å…§æ‰€æœ‰æª”æ¡ˆconcat

'''
def input_stream(path):
    all_data = []
    for file in os.listdir(path):
        if file.endswith(".xlsx") or file.endswith(".xls"):
            file_path = os.path.join(path, file)
            try:
                df = pd.read_excel(file_path, header=2, dtype=str)  # è®€å–ç‚ºå­—ä¸²é¿å…æ ¼å¼éŒ¯èª¤
                df.rename(columns={'Unnamed: 0': 'ç´¢å¼•'}, inplace=True)
                print(f"æˆåŠŸè®€å–æª”æ¡ˆ: {file}ï¼Œæ¬„ä½åç¨±: {df.columns.tolist()}")
                all_data.append(df)
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {file} å¤±æ•—: {e}")
    return pd.concat(all_data, ignore_index=True) if all_data else None




# **æ ¼å¼åŒ–ç´€éŒ„æ™‚é–“å‡½å¼** 
def format_datas(df,path1,path2):
    if df is None:
        return
    
    # åˆªé™¤ã€Œç´¢å¼•ã€æ¬„ä½
    df.drop(columns=["ç´¢å¼•"], errors="ignore", inplace=True)

    # è¨˜éŒ„åŸå§‹è³‡æ–™é‡
    initial_count = len(df)
    print(f"åŸå§‹è³‡æ–™é‡: {initial_count} ç­†")

    # è™•ç†ç´€éŒ„æ™‚é–“
    if "ç´€éŒ„æ™‚é–“" in df.columns:
        print("é–‹å§‹è™•ç†ç´€éŒ„æ™‚é–“æ•¸æ“š...")
        record_time = df["ç´€éŒ„æ™‚é–“"].astype(str).fillna("")
        merge_record = []
        temp_date = None
        for idx, value in enumerate(record_time):
            if len(value) == 19:
                temp_date = value.split(" ")[0]
            elif len(value) == 8:
                merge_record.append(f"{temp_date} {value}")
                merge_record.append("")
                temp_date = None
        df["ç´€éŒ„æ™‚é–“"] = merge_record
        print("ç´€éŒ„æ™‚é–“æ•¸æ“šè™•ç†å®Œæˆã€‚")
    else:
        print("è³‡æ–™ä¸­ä¸åŒ…å« 'ç´€éŒ„æ™‚é–“' æ¬„ä½ã€‚")

    # é‡æ–°æ’åˆ—æ¬„ä½
    reorder_data = df.reindex(columns=column_names, fill_value=np.nan)
    


    # æ¸…ç†ç•°å¸¸è»Šè™Ÿ
    pre_filter_count = len(reorder_data)
    reorder_data = reorder_data[
        (~reorder_data["è»Šè™Ÿ"].str.contains(r"\*", na=False)) &  # ç§»é™¤ `*`
        (reorder_data["è»Šè™Ÿ"].str.len().between(4, 7))  # ä¿ç•™é•·åº¦ 4~7 ç¢¼
    ]
    car_number_dropped = pre_filter_count - len(reorder_data)
    print(f"å› è»Šè™Ÿç•°å¸¸ï¼ˆé•·åº¦ä¸åœ¨ 4~7ï¼‰åˆªé™¤äº† {car_number_dropped} ç­†è³‡æ–™")

    

    # è½‰æ›é€²å…¥æ—¥èˆ‡å‡ºå ´æ—¥ç‚º datetimeï¼Œç„¶å¾Œæ ¼å¼åŒ–ç‚º "%Y/%m/%d"
    reorder_data["é€²å…¥æ—¥"] = pd.to_datetime(reorder_data["é€²å…¥æ—¥"], errors="coerce").dt.strftime("%Y/%m/%d")
    reorder_data["å‡ºå ´æ—¥"] = pd.to_datetime(reorder_data["å‡ºå ´æ—¥"], errors="coerce").dt.strftime("%Y/%m/%d")

    # é¿å… NaN å½±éŸ¿ï¼Œå°‡ "nan" è½‰ç‚º NaN
    reorder_data.replace("nan", np.nan, inplace=True)

    # è¨ˆç®—å…¨æ™‚é–“æ ¼å¼
    reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"] = pd.to_datetime(
        reorder_data["é€²å…¥æ—¥"] + " " + reorder_data["é€²å…¥æ™‚é–“"], 
        format="%Y/%m/%d %H:%M:%S", errors="coerce"
    )
    reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] = pd.to_datetime(
        reorder_data["å‡ºå ´æ—¥"] + " " + reorder_data["å‡ºå ´æ™‚é–“"], 
        format="%Y/%m/%d %H:%M:%S", errors="coerce"
    )

    '''
     --** å°‡åœç•™è¶…é7å¤©çš„ç”¨æˆ¶éƒ½å…ˆåˆªé™¤ **--
     --** æ™‚é–“ç©ºç™½è€…ä¹Ÿåˆªé™¤ **--
    '''
    # reorder_data.dropna(subset=["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“", "å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"])
    # reorder_data = reorder_data[reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] > reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"]].copy()
    # reorder_data = reorder_data[(reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] - reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"]) < pd.Timedelta(hours=168)].copy()

    # **ç¯©é¸é‚è¼¯**
    # ç§»é™¤é€²å…¥æˆ–å‡ºå ´æ™‚é–“ç‚ºç©ºçš„è³‡æ–™
    pre_filter_count = len(reorder_data)
    reorder_data = reorder_data.dropna(subset=["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“", "å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"]).copy()
    time_dropped = pre_filter_count - len(reorder_data)
    print(f"å› é€²å…¥æˆ–å‡ºå ´æ™‚é–“ç‚ºç©ºåˆªé™¤äº† {time_dropped} ç­†è³‡æ–™")

    # ç¢ºä¿å‡ºå ´æ™‚é–“æ™šæ–¼é€²å…¥æ™‚é–“
    pre_filter_count = len(reorder_data)
    reorder_data = reorder_data[reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] > reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"]].copy()
    invalid_time_dropped = pre_filter_count - len(reorder_data)
    print(f"å› å‡ºå ´æ™‚é–“æ—©æ–¼æˆ–ç­‰æ–¼é€²å…¥æ™‚é–“åˆªé™¤äº† {invalid_time_dropped} ç­†è³‡æ–™")

    # ç§»é™¤åœç•™å¤§æ–¼ç­‰æ–¼7å¤©çš„è³‡æ–™
    pre_filter_count = len(reorder_data)
    reorder_data = reorder_data[(reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] - reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"]) < pd.Timedelta(hours=168)].copy()
    long_stay_dropped = pre_filter_count - len(reorder_data)
    print(f"å› åœç•™æ™‚é–“è¶…é7å¤©åˆªé™¤äº† {long_stay_dropped} ç­†è³‡æ–™")

    # è¨ˆç®—ç¸½åˆªé™¤è³‡æ–™é‡åŠç™¾åˆ†æ¯”
    total_dropped = initial_count - len(reorder_data)
    dropped_percentage = (total_dropped / initial_count * 100) if initial_count > 0 else 0
    print(f"ç¸½å…±åˆªé™¤äº† {total_dropped} ç­†è³‡æ–™ï¼Œä½”åŸå§‹è³‡æ–™é‡çš„ {dropped_percentage:.2f}%")



    reorder_data["åœç•™æ™‚æ•¸"] = ((reorder_data["å…¨æ™‚é–“æ ¼å¼å‡ºå ´æ™‚é–“"] - reorder_data["å…¨æ™‚é–“æ ¼å¼é€²å…¥æ™‚é–“"]).dt.total_seconds()/3600).round(2)

    # æ ¡æ­£ 8/1 å‰çš„è»Šè™Ÿç¥¨ç¨®
    if "è»Šè™Ÿ" in reorder_data.columns and "ç¥¨ç¨®" in reorder_data.columns:
        reorder_data = reorder_data.apply(lambda row: correct_ticket_type(row,path1,path2), axis=1)
    

    # **è¼¸å‡º CSV**
    try:
        reorder_data.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"è™•ç†å®Œæˆï¼Œå·²å„²å­˜åˆ° {output_file}")
    except Exception as e:
        print(f"å„²å­˜æª”æ¡ˆ {output_file} å¤±æ•—: {e}")



ticket_verify1_path = os.path.join(directory_path,"112_confirm.xls")
ticket_verify2_path = os.path.join(directory_path,"113_confirm.xls")


# åŸ·è¡Œæµç¨‹
first_hlaf_mapping = load_ticket_mapping(ticket_verify1_path)
second_half_mapping = load_ticket_mapping(ticket_verify2_path)
# åˆä½µåŸå§‹è³‡æ–™
data = input_stream(input_file_path)

format_datas(data,first_hlaf_mapping,second_half_mapping)


# load_ticket_mapping -> è¼‰å…¥é€²å‡ºè³‡æ–™(make sure the ticket mapping corrrect)
# input stream -> format_datas 
